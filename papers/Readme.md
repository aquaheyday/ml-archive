# 📄 Papers

이 디렉터리는 머신러닝/딥러닝 관련 논문들을 정리한 공간입니다.  
읽은 논문의 핵심 요점, 개인 리뷰, 구현 코드 등을 정리하고 보관합니다.

---

## 📚 논문 목록
| 번호 | 논문 제목 | 주요 키워드 | 정리 링크 | 비고 |
|---|---|---|---|---|
| 1 | Attention is All You Need | Transformer, Self-Attention | [attention.md](./attention.md) | 필독 |
| 2 | BERT: Pre-training of Deep Bidirectional Transformers | NLP, Pre-training | [bert.md](./bert.md) | |
| 3 | XGBoost: A Scalable Tree Boosting System | Tree, Ensemble | [xgboost.md](./xgboost.md) | 실습 포함 |

---

## 📑 논문 정리 템플릿 (각 논문별 md 파일 템플릿 예시)

```md
# 논문 제목

- 📃 논문 링크: [Original Paper](https://arxiv.org/abs/xxx)
- ✍️ 저자: Name et al.
- 📅 발표 연도: 20XX
- 🔑 키워드: Transformer, Self-Attention, NLP

---

## 1. 배경 및 문제 정의
- 어떤 문제를 해결하려는 논문인지 간단 설명

## 2. 핵심 기여 (Contribution)
- 기존 연구와 차별점
- 새로운 방법론이나 아이디어 요약

## 3. 주요 내용 정리
- 모델 구조
- 학습 방식
- 실험 결과 요약
- 그림/도표 첨부 가능

## 4. 개인 리뷰 및 배운 점
- 인상 깊었던 포인트
- 한계점, 의문점
- 실제 코드 구현 가능 여부 등

## 5. 참고자료
- [Github 구현 링크](https://github.com/...)
- 관련 강의/블로그 링크 등
